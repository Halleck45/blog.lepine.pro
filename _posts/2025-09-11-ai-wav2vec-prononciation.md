---
layout: post
title: "Speech Embeddings et Pronunciation Detection : construire un pipeline IA local avec Wav2Vec2"
cover: "cover-speech-embeddings-et-pronunciation-detection-construire-un-pipeline-ia-local-avec-wav2vec2.png"
categories:
  - ai
tags:
  - ai
  - opensource
  - python
  - speech
status: publish
type: post
published: true
meta:
  _edit_last: '1'
  _syntaxhighlighter_encoded: '1'
en_permalink: /en/ai-wav2vec-pronunciation-vectorization/
tldr: |
  - Construire un coach de prononciation local avec Wav2Vec2 et Dynamic Time Warping.
  - Comprendre les bases : embeddings audio, distances, alignement temporel, phon√®mes et vis√®mes.
  - Un projet open source, concret et motivant pour progresser en anglais √† l'oral.

---

Lire ou √©crire ne me pose pas trop de probl√®mes, mais d√®s qu'il s'agit de **parler**, je me rends compte que ma prononciation n'est pas toujours claire.

ChatGPT est super pour le texte, mais les mod√®les g√©n√©ratifs actuels ne savent **pas √©valuer la prononciation**.  
Alors je me suis demand√© : **et si je construisais moi-m√™me un petit coach ?**

Un outil qui :
- m'√©coute,
- compare ce que je dis √† une r√©f√©rence,
- et me renvoie un retour clair et visuel.

C'est ce que je vais d√©tailler ici, tout en expliquant simplement les briques IA derri√®re : **embeddings, distances, DTW, phon√®mes, vis√®mes**.



## Pourquoi comparer deux sons est si difficile ?

Un mot parl√© n'est pas une suite de lettres, mais une **onde sonore qui varie dans le temps** :

- l'air vibre,
- la voix monte et descend,
- certaines parties sont longues, d'autres tr√®s courtes.

M√™me deux personnes qui prononcent *exactement* le m√™me mot n'auront jamais deux courbes identiques.  
La question centrale devient donc : **comment comparer deux audios qui ne s'alignent pas parfaitement ?**


## L'approche choisie

Voici un aper√ßu de l'architecture que j'ai mise en place :

<p align="center">
    <img src="/images/2025-08-audio-embedding.png" alt="Embedding architecture" width="600px">
</p>


N'ayez pas peur, on va expliquer tous ces concepts pas √† pas.

## Transformer le son en nombres : les embeddings

Un ordinateur ne comprend pas les sons. Il ne manipule que des **vecteurs de nombres**.

- Un **vecteur** = une liste de nombres, par ex. `[0.2, -0.7, 1.1]`.
- Un **embedding audio** = une repr√©sentation condens√©e d'un petit morceau de son.

Avec **Wav2Vec2**, chaque tranche d'audio de quelques millisecondes est encod√©e en **768 nombres** d√©crivant le timbre, l'√©nergie, l'articulation, etc.

```python
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h")

def extract_embeddings(audio_waveform, sampling_rate=16000):
    inputs = processor(audio_waveform, sampling_rate=sampling_rate,
                       return_tensors="pt", padding=True)
    input_values = inputs.input_values.squeeze(0)
    with torch.no_grad():
        features = model(input_values).last_hidden_state
    return features.squeeze(0).numpy()
```

En r√©sum√© :
- **sons proches ‚Üí vecteurs proches**,
- **sons diff√©rents ‚Üí vecteurs √©loign√©s**.



## Mesurer la ressemblance : les distances

Une fois deux vecteurs extraits, il faut mesurer leur **proximit√©**.

L'outil de base : la **distance euclidienne**.  
Exemple simple entre `[1,2]` et `[4,6]` :

```
‚àö((4-1)¬≤ + (6-2)¬≤) = 5
```

Avec les embeddings audio, c'est le m√™me principe mais dans un espace √† 768 dimensions.

```python
from fastdtw import fastdtw
from scipy.spatial.distance import euclidean

def compare_pronunciation(expected, actual):
    expected_seq = get_phoneme_embeddings(expected)
    actual_seq = get_phoneme_embeddings(actual)
    distance, _ = fastdtw(expected_seq, actual_seq, dist=euclidean)
    return distance
```



## DTW : quand on ne parle pas √† la m√™me vitesse

Probl√®me : un mot peut durer **0,5 seconde** chez moi et **0,8 seconde** dans la r√©f√©rence.  
Si on compare na√Øvement, √ßa √©choue.

La solution : **Dynamic Time Warping (DTW)**.  
Cet algorithme aligne deux s√©quences de vitesses diff√©rentes en "√©tirant" ou "compressant" le temps pour faire correspondre les parties similaires.

```python
import numpy as np

def align_sequences_dtw(seq1, seq2):
    distance, path = fastdtw(seq1, seq2, dist=euclidean)
    aligned1, aligned2 = [], []
    for i, j in path:
        aligned1.append(seq1[i][0])
        aligned2.append(seq2[j][0])
    return np.array(aligned1), np.array(aligned2)
```

R√©sultat : une comparaison robuste, m√™me si le rythme diff√®re.



## Phon√®mes et vis√®mes : entendre ET voir

Certains sons sont difficiles √† distinguer √† l'oreille.  
Exemple : *"think"* vs *"sink"* (diff√©rence subtile entre /Œ∏/ et /s/).

- Un **phon√®me** = le plus petit son distinctif (ex. /p/, /a/, /t/).
- Un **vis√®me** = l'image de la bouche qui produit ce son.

Dans mon prototype, quand un mot est mal prononc√©, je peux cliquer dessus et voir une **animation de bouche** qui montre la bonne articulation.

üëâ Apprentissage plus concret : j'entends **et je vois** ce qu'il faut corriger.  
([Documentation Microsoft sur les vis√®mes](https://learn.microsoft.com/fr-fr/azure/ai-services/speech-service/how-to-speech-synthesis-viseme?tabs=visemeid&pivots=programming-language-csharp))



## Le pipeline complet

<p align="center">
    <img src="/images/2025-08-audio-analysis.png" alt="pronunciation pipeline" width="600px">
</p>

1. J'√©cris un texte : _"Hello, how are you?"_.
2. Le syst√®me g√©n√®re une **voix de r√©f√©rence** (ex. gTTS).
3. J'enregistre ma voix.
4. Extraction des **embeddings Wav2Vec2** pour les deux audios.
5. Alignement avec **DTW**.
6. Comparaison **phon√®me par phon√®me**.
7. Feedback renvoy√© :
    - un **score global** (sur 100),
    - une liste de mots mal prononc√©s,
    - un feedback clair : "‚ùå Tu dois mieux prononcer : *Hello, you*",
    - et les vis√®mes correspondants.



## Limites de l'approche

Soyons honn√™tes :

- **Wav2Vec2 n'est pas entra√Æn√© pour noter la prononciation** ‚Üí approximation.
- **Voix de synth√®se imparfaite** ‚Üí accent parfois artificiel.
- **Vis√®mes simplifi√©s** ‚Üí animations trop discr√®tes compar√©es √† une bouche r√©elle.
- **Subjectivit√© persistante** ‚Üí la compr√©hension d'un natif reste la r√©f√©rence ultime.

## Ce que √ßa apporte malgr√© tout

- Je peux **m'√©couter objectivement**.
- J'identifie **mes erreurs** sans prof.
- Je dispose d'un **feedback visuel**.
- Et surtout : je garde la **motivation** √† pratiquer.



## Pistes pour aller plus loin

- Int√©grer des mod√®les sp√©cialis√©s de **pronunciation scoring** (API commerciales).
- Am√©liorer les vis√®mes avec de la **3D fluide**.
- Ajouter la **prosodie** (intonation, accentuation, rythme).
- √âtendre √† d'autres langues.
- √âvaluer des phrases longues (lecture, dialogue).



## Conclusion

Je n'ai pas r√©invent√© Duolingo. Mais j'ai construit un **coach maison** qui m'aide √† progresser √† l'oral.

La force vient de la combinaison :

- **IA** (Wav2Vec2) pour transformer la voix en vecteurs,
- **algorithmes classiques** (DTW) pour comparer,
- **visuels p√©dagogiques** (vis√®mes) pour corriger.

Un m√©lange de machine learning, de maths et de p√©dagogie, au service d'un objectif tr√®s concret : **mieux parler anglais**.

üëâ Le code source est disponible sur [GitHub](https://github.com/Halleck45/OpenPronounce).  
Vos retours ou contributions sont bienvenus.

