---
layout: post
title: "IA : Wav2Vec2, distances et vis√®mes expliqu√©s simplement"
cover: "cover-parser-du-code-php-sans-d-pendre-de-php.png"
categories:
  - ai
tags:
  - ai
  - opensource
  - python
status: publish
type: post
published: true
meta:
  _edit_last: '1'
  _syntaxhighlighter_encoded: '1'
en_permalink: /en/ai-wav2vec-prononciation/

---


Lire ou √©crire ne me pose pas trop de probl√®mes, mais d√®s qu'il s'agit de parler, je me rends compte que ma prononciation n'est pas toujours claire.

ChatGPT est super fort pour du texte, mais les mod√®les d'IA g√©n√©rative ne sont pas du tout adapt√©s pour de l'analyse de prononciation.

Alors je me suis demand√© : **et si je construisais moi-m√™me un petit coach de prononciation ?**

Un outil qui m'√©coute, qui compare ce que je dis √† la bonne prononciation, et qui me donne un retour clair.

C'est le projet que je vais vous pr√©senter ici. Mais surtout, je vais en profiter pour vulgariser les concepts derri√®re : _embeddings, distances, DTW, phon√®mes, vis√®mes_. Parce que ce vocabulaire peut sembler opaque, alors qu'en fait, c'est assez accessible si on prend le temps.

## Pourquoi comparer des sons est difficile

Quand on dit un mot, ce n'est pas une simple suite de lettres. C'est une **onde sonore** qui varie dans le temps :

-   l'air vibre,
-   la voix monte et descend,
-   certaines parties sont longues, d'autres tr√®s rapides.

Deux personnes qui prononcent _exactement_ le m√™me mot n'auront jamais une courbe audio identique.

üëâ C'est √ßa la difficult√© : **comment comparer deux audios qui ne sont pas strictement identiques ?**

## L'approche choisie

Voici un aper√ßu de l'architecture que j'ai mise en place :

<p align="center">
    <img src="/images/2025-08-audio-embedding.png" alt="Embedding architecture" width="600px">
</p>

N'ayez pas peur, on va expliquer tous ces concepts pas √† pas.


## Les vecteurs et les embeddings : transformer le son en nombres

Les ordinateurs ne ‚Äúcomprennent‚Äù pas les sons. Ils comprennent les nombres. Donc, premi√®re √©tape : convertir la voix en une repr√©sentation num√©rique.

-   Un **vecteur**, c'est une liste de nombres. Par exemple : `[0.1, 0.3, -0.7]`.
-   Un **embedding**, c'est une repr√©sentation de quelque chose (ici, un bout d'audio) sous forme de vecteur.

Imaginez que chaque petit morceau de son (quelques millisecondes) soit r√©sum√© par 768 nombres. Ces nombres d√©crivent les caract√©ristiques du son : timbre, √©nergie, articulation‚Ä¶

C'est ce que fait le mod√®le **Wav2Vec2** (de Facebook/Meta). Il prend de l'audio en entr√©e et produit une suite d'**embeddings** : un grand tableau de nombres qui r√©sume comment le son √©volue dans le temps.

En simplifiant, on peut dire que :

-   deux sons proches produiront des vecteurs proches,
-   deux sons tr√®s diff√©rents produiront des vecteurs √©loign√©s.

```python
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h")

def extract_embeddings(audio_waveform, sampling_rate=16000):
    """
    Extrait les embeddings bruts de Wav2Vec2 pour une entr√©e audio donn√©e.
    """

    # Transformer l'audio en entr√©e pour Wav2Vec2
    inputs = processor(audio_waveform, sampling_rate=sampling_rate, return_tensors="pt", padding=True)

    # V√©rifier la forme avant d'envoyer au mod√®le
    input_values = inputs.input_values
    if len(input_values.shape) > 2:  # Supprimer les dimensions inutiles
        input_values = input_values.squeeze(0)

    with torch.no_grad():
        features = model(input_values).last_hidden_state  # (batch, time, features)

    return features.squeeze(0).numpy()
```

## Mesurer la ressemblance : les distances

Une fois qu'on a deux vecteurs (un pour ma prononciation, un pour la r√©f√©rence), il faut mesurer √† quel point ils se ressemblent.

C'est l√† qu'interviennent les **distances**.

La plus simple est la **distance euclidienne** (comme la distance entre deux points dans un plan).

Exemple : entre `[1, 2]` et `[4, 6]`, la distance vaut ‚àö((4-1)¬≤ + (6-2)¬≤) = 5.

Avec les embeddings, c'est pareil, sauf qu'au lieu de 2 dimensions, on en a 768. Mais le principe reste identique : plus la distance est petite, plus les sons sont proches.

```python
from fastdtw import fastdtw
from scipy.spatial.distance import euclidean

def compare_pronunciation(expected, actual):
    """ Compare la prononciation avec DTW et retourne un score """
    expected_seq = get_phoneme_embeddings(expected)
    actual_seq = get_phoneme_embeddings(actual)

    distance, _ = fastdtw(expected_seq, actual_seq, dist=euclidean)
    
    return distance
```

## DTW : comparer quand on ne parle pas √† la m√™me vitesse

Probl√®me : je peux dire _hello_ en 0,5 seconde, et la voix de synth√®se peut le dire en 0,8 seconde. Si je compare na√Øvement les vecteurs, √ßa ne marche pas.

C'est pour √ßa qu'on utilise le **Dynamic Time Warping (DTW)**.

L'id√©e :

-   On aligne deux s√©quences qui n'ont pas la m√™me vitesse.
-   Par exemple, si je dis ‚Äúhe-llo‚Äù en deux temps, et la voix de synth√®se dit ‚Äúhe-llo‚Äù en trois temps, DTW va faire correspondre mes deux syllabes avec leurs trois syllabes.

C'est comme si on √©tirait ou compressait le temps pour superposer les deux courbes.

R√©sultat : on obtient une comparaison beaucoup plus robuste.

```python
import numpy as np

def align_sequences_dtw(seq1, seq2):
    """
    Aligne deux s√©quences de valeurs num√©riques en utilisant Dynamic Time Warping (DTW).
    Retourne les s√©quences interpol√©es pour avoir la m√™me longueur.
    Ca permet de comparer les deux s√©quences plus facilement, car par exemple l'une peut √™tre plus rapide que l'autre, 
    ou plus courte
    """
    distance, path = fastdtw(seq1, seq2, dist=euclidean)
    
    aligned_seq1 = []
    aligned_seq2 = []

    for i, j in path:
        aligned_seq1.append(seq1[i][0]) 
        aligned_seq2.append(seq2[j][0])

    # on peut amplifier si besoin artificiellement la diff√©rence, sinon souvent les deux courbes se superposent
    #aligned_seq2 = aligned_seq2 + (aligned_seq2 - aligned_seq1) * 2  # Amplifier la diff√©rence

    return np.array(aligned_seq1), np.array(aligned_seq2)
```


## Phon√®mes et vis√®mes : du son au mouvement de la bouche

Un autre probl√®me de la prononciation : parfois, on entend mal la diff√©rence.

Prenez _‚Äúthink‚Äù_ et _‚Äúsink‚Äù_. Si vous n'avez pas l'habitude, la diff√©rence entre /Œ∏/ et /s/ est subtile.

C'est pour √ßa que j'ai ajout√© une deuxi√®me dimension √† mon projet : **les vis√®mes**.

-   Un **phon√®me** est le plus petit son d'une langue (ex : /p/, /a/, /t/).
-   Un **vis√®me** est l'image de la bouche qui produit ce son.

Concr√®tement, dans mon interface, quand un mot est mal prononc√©, je peux cliquer dessus et voir une petite bouche qui s'anime pour montrer la bonne position.

üëâ √áa rend l'apprentissage plus concret, parce que je vois _et_ j'entends ce que je dois corriger.

Microsoft a publi√© de [tr√®s bons articles sur le sujet](https://learn.microsoft.com/fr-fr/azure/ai-services/speech-service/how-to-speech-synthesis-viseme?tabs=visemeid&pivots=programming-language-csharp)

## Comment tout √ßa s'encha√Æne

<p align="center">
    <img src="/images/2025-08-audio-analysis.png" alt="linguistic analysis" width="600px">
</p>

1.  Je tape un texte, par exemple : _‚ÄúHello, how are you?‚Äù_.
2.  Le syst√®me g√©n√®re une **voix de r√©f√©rence** (via gTTS).
3.  J'enregistre ma voix en r√©p√©tant la phrase.
4.  On extrait les embeddings avec **Wav2Vec2** pour les deux audios.
5.  On aligne avec **DTW** et on calcule les distances.
6.  On **transcrit** ma voix automatiquement, puis on compare phon√®me par phon√®me.
7.  Le syst√®me me renvoie :

-   un **score global** sur 100,
-   une liste de mots mal prononc√©s,
-   un feedback (‚Äú‚ùå Tu dois mieux prononcer : Hello, you‚Äù),
-   et, en bonus, les vis√®mes correspondants.

## Les limites de l'approche

Soyons honn√™tes : ce n'est pas magique.

-   **Wav2Vec2 n'est pas fait pour √ßa.** Il reconna√Æt la parole, mais n'a pas √©t√© entra√Æn√© pour noter la prononciation. La distance mesur√©e peut √™tre utile, mais elle n'√©quivaut pas √† l'oreille d'un professeur.
-   **La voix de synth√®se n'est pas parfaite.** C'est propre, mais pas toujours naturel.
-   **Les vis√®mes sont simplifi√©s.** Dans la vraie vie, la bouche bouge de mani√®re fluide et les sons s'encha√Ænent. Ici, on affiche des images assez ‚Äúdiscr√®tes‚Äù.
-   **La subjectivit√© reste.** Ce qui compte, ce n'est pas seulement la proximit√© acoustique, mais la compr√©hension par un auditeur humain.

## Ce que √ßa apporte malgr√© tout

Malgr√© ces limites, je trouve ce projet tr√®s utile pour moi :

-   Je peux **m'√©couter objectivement**.
-   Je vois **o√π je me trompe**, sans avoir besoin d'un natif en face de moi.
-   J'ai un **feedback visuel** pour placer ma bouche correctement.
-   Et surtout, je garde la **motivation**.

## Et apr√®s ?

Pour aller plus loin, il y aurait plein de pistes :

-   Utiliser des mod√®les sp√©cialis√©s en **pronunciation scoring** (il en existe dans certaines API commerciales).
-   Am√©liorer l'animation des vis√®mes avec de la 3D et de la coarticulation.
-   Ajouter des √©l√©ments de **prosodie** : intonation, accentuation, rythme.
-   √âtendre √† d'autres langues que l'anglais.

Mais m√™me dans sa forme actuelle, ce petit coach maison me pousse √† m'exercer r√©guli√®rement. Et c'est d√©j√† √©norme. En plus c'est agr√©able 
√† coder !

## Conclusion

Je ne pr√©tends pas avoir r√©invent√© Duolingo, mais ce projet m'a permis de mieux comprendre mes erreurs et de progresser √† l'oral.

Ce qui me pla√Æt surtout, c'est la combinaison :

-   de l'**IA** pour transformer ma voix en vecteurs,
-   des **algorithmes simples** comme DTW pour comparer,
-   et une **visualisation p√©dagogique** avec les vis√®mes.

Bref : un m√©lange de math√©matiques, de machine learning, et de p√©dagogie‚Ä¶ au service d'un objectif tr√®s personnel : mieux parler anglais.

Le code source est disponible sur [GitHub](https://github.com/Halleck45/OpenPronounce). N'h√©sitez pas √† partager vos retours ou √† contribuer !